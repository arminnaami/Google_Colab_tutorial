{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Google_Colab_tutorial.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1hjKE6QCD9ez","colab_type":"text"},"source":["## Intro.\n","\n","It is example codes and script for explaining about Google Colab. \n","\n","My ways are not the optimal way to use Colab, but I think these are helpful enough to increase experiment speed.\n","\n","\n","## What is Google Colab.\n","https://www.youtube.com/watch?v=inN8seMm7UI\n","\n","Google Colab(GC) is **free**  machine learning tool for studying and researching.\n","\n","It means that you can run your codes in GC, which contains a powerful GPU machine.\n","\n","and its only limitation is the maximum running time, which is 12 hours.\n","\n","and it is connected to your Google drive so you can access GC GPU anytime.\n","\n","So there is no reason not to use GC."]},{"cell_type":"markdown","metadata":{"id":"h3udZ7cCG2a6","colab_type":"text"},"source":["# How to Use GC\n","\n","## Connect with your Google drive\n","\n","If you want to use GC, Google drive is needed for saving codes, datasets, training log, and so on.\n","\n","so the first step is connecting to Google Driver.\n","\n","\n","Look below code"]},{"cell_type":"code","metadata":{"id":"odw4J82VExjd","colab_type":"code","outputId":"502728ac-afcc-443a-db4c-868be8909aa4","executionInfo":{"status":"ok","timestamp":1563334789346,"user_tz":-540,"elapsed":22324,"user":{"displayName":"이승현","photoUrl":"","userId":"07009177712654713303"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cleyIr7JITja","colab_type":"text"},"source":["this code connect your google drive to GC\n","\n","it is run by ctrl + Enter.\n","\n","if you run this code, return some URL and you can fine authorization code in there.\n","\n","After printed \n","\n","\"Mounted at /content/gdrive\"\n","\n","you can access you google driver in GC like below image and your home path is set to ''/content'' \n","\n","![](https://drive.google.com/uc?id=1bmWrRZYzyb1b4WkaXt9a9bgM9cTEoLyk)\n","\n","\n","It is Korean, but the button's location is the same I think\n","\n","Next, I'll make some folder in my Google drive to upload new codes by the below code.\n","\n","As you can see if you add '!' in front of code line, it runs in the terminal\n"]},{"cell_type":"code","metadata":{"id":"Fa57dSchNeJj","colab_type":"code","outputId":"692ad6f7-2e1b-4971-8bd7-a0773d363052","executionInfo":{"status":"ok","timestamp":1563331939972,"user_tz":-540,"elapsed":4674,"user":{"displayName":"이승현","photoUrl":"","userId":"07009177712654713303"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!mkdir gdrive/My\\ Drive/testing_folder"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘gdrive/My Drive/testing_folder’: File exists\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EWpl_eUDPPVe","colab_type":"text"},"source":["![](https://drive.google.com/uc?id=1PvNolorXaIgs9ls9ldvCypa-hdtfj5aX)\n","\n","\n","As you can see, a new folder is made in Google drive.\n","\n","it is possible to make a new folder in Google driver surely, but I want to show that it is not so different from our computer.\n","\n","\n","## Upload codes in Google Colab\n","\n","The next step is uploading a bunch of codes. I'll explain how to code in GC, but it is not so comfortable.\n","\n","So if you have to modify many things, work on your computer is better.\n","\n","if you upload codes in the folder, you can see in GC also.\n","\n","\n","![](https://drive.google.com/uc?id=1w1_xa1apDJhe2xcoXFuZBHJEA-16bBMe)\n","\n","\n","## GPU setting\n","You surely want to run your codes in GPU. So you have to change the runtime type.\n","\n","![](https://drive.google.com/uc?id=17X0FqZzLCjEeJhn_13WPZFnb6McPcMzH)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"8SfVIbgHVks4","colab_type":"code","outputId":"5155e958-90a2-4366-b70f-c51892c567a1","colab":{"base_uri":"https://localhost:8080/","height":335}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Wed Apr 24 17:13:22 2019       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mi2xNhzFVwRp","colab_type":"text"},"source":["it's done, and Tesla T4 is yours!\n","\n","## Start Tensorboard\n","I think analyzing the learning plot is very important. so I'll explain about how to run Tensorboard in GC\n","\n","I set training path to\" test\"\n","\n","So Tensorboard is run by the below codes."]},{"cell_type":"code","metadata":{"id":"yUb-PUKwXIMv","colab_type":"code","outputId":"0d4996d2-b01b-4b97-8b03-6c3ff57014a0","executionInfo":{"status":"ok","timestamp":1563334890234,"user_tz":-540,"elapsed":7771,"user":{"displayName":"이승현","photoUrl":"","userId":"07009177712654713303"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2019-07-17 03:41:25--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 35.173.6.94, 52.21.103.149, 52.22.145.207, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|35.173.6.94|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17556757 (17M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  16.74M  15.5MB/s    in 1.1s    \n","\n","2019-07-17 03:41:26 (15.5 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [17556757/17556757]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sjnnUfeDauZK","colab_type":"code","outputId":"18f2d193-6a96-49a8-d987-8bde21d06bd6","executionInfo":{"status":"ok","timestamp":1563334893805,"user_tz":-540,"elapsed":5499,"user":{"displayName":"이승현","photoUrl":"","userId":"07009177712654713303"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["LOG_DIR = 'test'\n","\n","get_ipython().system_raw(\n","    'tensorboard --logdir={} --host=0.0.0.0 --port=6006 &'\n","    .format(LOG_DIR))\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["https://a9fe7509.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dTcypKPuLgfJ","colab_type":"text"},"source":["when all process is done, you can see some link in last line and the page show Tensorboard\n","\n","you can check running of Tensorboard by the process manager"]},{"cell_type":"code","metadata":{"id":"x4iKnACILxSx","colab_type":"code","outputId":"60cec788-3792-444d-ed7c-760704242da9","executionInfo":{"status":"ok","timestamp":1563334842965,"user_tz":-540,"elapsed":3449,"user":{"displayName":"이승현","photoUrl":"","userId":"07009177712654713303"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["!ps -A"],"execution_count":6,"outputs":[{"output_type":"stream","text":["    PID TTY          TIME CMD\n","      1 ?        00:00:00 run.sh\n","     10 ?        00:00:01 node\n","     25 ?        00:00:03 jupyter-noteboo\n","    118 ?        00:00:00 tail\n","    127 ?        00:00:07 python3\n","    220 ?        00:00:00 drive\n","    294 ?        00:00:00 drive\n","    342 ?        00:00:00 tail\n","    343 ?        00:00:00 grep\n","    364 ?        00:00:02 tensorboard\n","    366 ?        00:00:00 ngrok\n","    388 ?        00:00:00 ps\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UETRQM251RKB"},"source":["As you can see, Tensorboard is running backend.\n","\n","## Training the network\n","\n","Surely, you need some codes for training, but if you don't have, you can clone the example codes by below cell.\n"]},{"cell_type":"code","metadata":{"id":"KDGfyE_v1sxo","colab_type":"code","outputId":"bf703db2-06f3-4246-bc3c-69137d3707b7","executionInfo":{"status":"ok","timestamp":1563334900460,"user_tz":-540,"elapsed":4611,"user":{"displayName":"이승현","photoUrl":"","userId":"07009177712654713303"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["!git clone https://github.com/sseung0703/Google_Colab_tutorial.git"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'Google_Colab_tutorial'...\n","remote: Enumerating objects: 21, done.\u001b[K\n","remote: Counting objects:   4% (1/21)   \u001b[K\rremote: Counting objects:   9% (2/21)   \u001b[K\rremote: Counting objects:  14% (3/21)   \u001b[K\rremote: Counting objects:  19% (4/21)   \u001b[K\rremote: Counting objects:  23% (5/21)   \u001b[K\rremote: Counting objects:  28% (6/21)   \u001b[K\rremote: Counting objects:  33% (7/21)   \u001b[K\rremote: Counting objects:  38% (8/21)   \u001b[K\rremote: Counting objects:  42% (9/21)   \u001b[K\rremote: Counting objects:  47% (10/21)   \u001b[K\rremote: Counting objects:  52% (11/21)   \u001b[K\rremote: Counting objects:  57% (12/21)   \u001b[K\rremote: Counting objects:  61% (13/21)   \u001b[K\rremote: Counting objects:  66% (14/21)   \u001b[K\rremote: Counting objects:  71% (15/21)   \u001b[K\rremote: Counting objects:  76% (16/21)   \u001b[K\rremote: Counting objects:  80% (17/21)   \u001b[K\rremote: Counting objects:  85% (18/21)   \u001b[K\rremote: Counting objects:  90% (19/21)   \u001b[K\rremote: Counting objects:  95% (20/21)   \u001b[K\rremote: Counting objects: 100% (21/21)   \u001b[K\rremote: Counting objects: 100% (21/21), done.\u001b[K\n","remote: Compressing objects:   5% (1/20)   \u001b[K\rremote: Compressing objects:  10% (2/20)   \u001b[K\rremote: Compressing objects:  15% (3/20)   \u001b[K\rremote: Compressing objects:  20% (4/20)   \u001b[K\rremote: Compressing objects:  25% (5/20)   \u001b[K\rremote: Compressing objects:  30% (6/20)   \u001b[K\rremote: Compressing objects:  35% (7/20)   \u001b[K\rremote: Compressing objects:  40% (8/20)   \u001b[K\rremote: Compressing objects:  45% (9/20)   \u001b[K\rremote: Compressing objects:  50% (10/20)   \u001b[K\rremote: Compressing objects:  55% (11/20)   \u001b[K\rremote: Compressing objects:  60% (12/20)   \u001b[K\rremote: Compressing objects:  65% (13/20)   \u001b[K\rremote: Compressing objects:  70% (14/20)   \u001b[K\rremote: Compressing objects:  75% (15/20)   \u001b[K\rremote: Compressing objects:  80% (16/20)   \u001b[K\rremote: Compressing objects:  85% (17/20)   \u001b[K\rremote: Compressing objects:  90% (18/20)   \u001b[K\rremote: Compressing objects:  95% (19/20)   \u001b[K\rremote: Compressing objects: 100% (20/20)   \u001b[K\rremote: Compressing objects: 100% (20/20), done.\u001b[K\n","Unpacking objects:   4% (1/21)   \rUnpacking objects:   9% (2/21)   \rUnpacking objects:  14% (3/21)   \rremote: Total 21 (delta 6), reused 11 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects:  19% (4/21)   \rUnpacking objects:  23% (5/21)   \rUnpacking objects:  28% (6/21)   \rUnpacking objects:  33% (7/21)   \rUnpacking objects:  38% (8/21)   \rUnpacking objects:  42% (9/21)   \rUnpacking objects:  47% (10/21)   \rUnpacking objects:  52% (11/21)   \rUnpacking objects:  57% (12/21)   \rUnpacking objects:  61% (13/21)   \rUnpacking objects:  66% (14/21)   \rUnpacking objects:  71% (15/21)   \rUnpacking objects:  76% (16/21)   \rUnpacking objects:  80% (17/21)   \rUnpacking objects:  85% (18/21)   \rUnpacking objects:  90% (19/21)   \rUnpacking objects:  95% (20/21)   \rUnpacking objects: 100% (21/21)   \rUnpacking objects: 100% (21/21), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iNnFmnv01twj","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"L9Fq_uduYMCi","colab_type":"code","cellView":"code","outputId":"925d358c-f6d5-4bc2-f991-da88abef3848","executionInfo":{"status":"ok","timestamp":1556156930731,"user_tz":-540,"elapsed":81927,"user":{"displayName":"이승현","photoUrl":"","userId":"07009177712654713303"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python /content/Google_Colab_tutorial/train.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0717 03:41:47.181896 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:3: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0717 03:41:47.183085 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:209: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","W0717 03:41:47.183602 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:29: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","W0717 03:41:47.183737 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:29: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n","\n","Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 6s 0us/step\n","W0717 03:41:55.682116 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:39: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0717 03:41:55.688075 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:187: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","W0717 03:41:55.728328 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:194: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n","\n","W0717 03:41:57.737185 140446799050624 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","W0717 03:41:57.737626 140446799050624 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:2563: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0717 03:41:57.741732 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:48: The name tf.train.create_global_step is deprecated. Please use tf.compat.v1.train.create_global_step instead.\n","\n","W0717 03:41:57.746896 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:49: The name tf.floor_div is deprecated. Please use tf.math.floordiv instead.\n","\n","W0717 03:41:57.769912 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:205: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","W0717 03:41:58.293776 140446799050624 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","W0717 03:41:58.782892 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:81: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n","\n","W0717 03:41:58.820420 140446799050624 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0717 03:41:58.829431 140446799050624 deprecation.py:323] From /content/Google_Colab_tutorial/train.py:82: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0717 03:41:58.833737 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:90: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n","\n","W0717 03:41:58.833888 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:93: The name tf.losses.get_regularization_losses is deprecated. Please use tf.compat.v1.losses.get_regularization_losses instead.\n","\n","W0717 03:41:59.128669 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:103: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n","\n","W0717 03:41:59.132760 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:113: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","2019-07-17 03:41:59.220830: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n","2019-07-17 03:41:59.223137: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1941480 executing computations on platform Host. Devices:\n","2019-07-17 03:41:59.223198: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n","2019-07-17 03:41:59.227930: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n","2019-07-17 03:41:59.453036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-07-17 03:41:59.453569: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x193fdc0 executing computations on platform CUDA. Devices:\n","2019-07-17 03:41:59.453596: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2019-07-17 03:41:59.453799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-07-17 03:41:59.454138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n","name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n","pciBusID: 0000:00:04.0\n","2019-07-17 03:41:59.466331: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-07-17 03:41:59.653115: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2019-07-17 03:41:59.733087: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n","2019-07-17 03:41:59.754740: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n","2019-07-17 03:41:59.937314: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n","2019-07-17 03:42:00.058468: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n","2019-07-17 03:42:00.402790: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","2019-07-17 03:42:00.402988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-07-17 03:42:00.403457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-07-17 03:42:00.403814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n","2019-07-17 03:42:00.406189: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-07-17 03:42:00.408299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2019-07-17 03:42:00.408327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n","2019-07-17 03:42:00.408339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n","2019-07-17 03:42:00.410397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-07-17 03:42:00.410919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-07-17 03:42:00.411297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n","2019-07-17 03:42:02.347188: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n","2019-07-17 03:42:03.128527: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n","W0717 03:42:06.225337 140446799050624 deprecation_wrapper.py:119] From /content/Google_Colab_tutorial/train.py:157: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","I0717 03:42:06.225561 140446799050624 train.py:159] Epoch 000 Step 000001 - train_Accuracy : 8.59%  val_Accuracy : 14.36%\n","I0717 03:42:07.672256 140446799050624 train.py:173] global step 000200: loss = 5.6034 (0.026 sec/step)\n","I0717 03:42:09.200307 140446799050624 train.py:159] Epoch 001 Step 000391 - train_Accuracy : 30.60%  val_Accuracy : 39.40%\n","I0717 03:42:09.262407 140446799050624 train.py:173] global step 000400: loss = 2.8467 (0.007 sec/step)\n","I0717 03:42:10.636175 140446799050624 train.py:173] global step 000600: loss = 2.4707 (0.007 sec/step)\n","I0717 03:42:12.104272 140446799050624 train.py:159] Epoch 002 Step 000782 - train_Accuracy : 38.86%  val_Accuracy : 44.99%\n","I0717 03:42:12.231223 140446799050624 train.py:173] global step 000800: loss = 2.2294 (0.007 sec/step)\n","I0717 03:42:13.646749 140446799050624 train.py:173] global step 001000: loss = 2.1765 (0.007 sec/step)\n","I0717 03:42:15.057626 140446799050624 train.py:159] Epoch 003 Step 001172 - train_Accuracy : 42.10%  val_Accuracy : 49.28%\n","I0717 03:42:15.247827 140446799050624 train.py:173] global step 001200: loss = 2.0323 (0.007 sec/step)\n","I0717 03:42:16.638812 140446799050624 train.py:173] global step 001400: loss = 1.9578 (0.007 sec/step)\n","I0717 03:42:17.988308 140446799050624 train.py:159] Epoch 004 Step 001563 - train_Accuracy : 45.26%  val_Accuracy : 50.71%\n","I0717 03:42:18.245134 140446799050624 train.py:173] global step 001600: loss = 1.9043 (0.007 sec/step)\n","I0717 03:42:19.648451 140446799050624 train.py:173] global step 001800: loss = 1.8518 (0.007 sec/step)\n","I0717 03:42:20.936945 140446799050624 train.py:159] Epoch 005 Step 001954 - train_Accuracy : 46.95%  val_Accuracy : 49.80%\n","I0717 03:42:21.255783 140446799050624 train.py:173] global step 002000: loss = 1.8194 (0.007 sec/step)\n","I0717 03:42:22.674643 140446799050624 train.py:173] global step 002200: loss = 1.7775 (0.007 sec/step)\n","I0717 03:42:23.927339 140446799050624 train.py:159] Epoch 006 Step 002344 - train_Accuracy : 48.77%  val_Accuracy : 52.06%\n","I0717 03:42:24.317273 140446799050624 train.py:173] global step 002400: loss = 1.7587 (0.007 sec/step)\n","I0717 03:42:25.718669 140446799050624 train.py:173] global step 002600: loss = 1.6980 (0.007 sec/step)\n","I0717 03:42:26.881703 140446799050624 train.py:159] Epoch 007 Step 002735 - train_Accuracy : 50.11%  val_Accuracy : 55.32%\n","I0717 03:42:27.331837 140446799050624 train.py:173] global step 002800: loss = 1.6877 (0.007 sec/step)\n","I0717 03:42:28.726646 140446799050624 train.py:173] global step 003000: loss = 1.6752 (0.007 sec/step)\n","I0717 03:42:29.832882 140446799050624 train.py:159] Epoch 008 Step 003125 - train_Accuracy : 51.59%  val_Accuracy : 55.99%\n","I0717 03:42:30.356438 140446799050624 train.py:173] global step 003200: loss = 1.6376 (0.007 sec/step)\n","I0717 03:42:31.746024 140446799050624 train.py:173] global step 003400: loss = 1.6271 (0.007 sec/step)\n","I0717 03:42:32.779225 140446799050624 train.py:159] Epoch 009 Step 003516 - train_Accuracy : 52.73%  val_Accuracy : 55.82%\n","I0717 03:42:33.369377 140446799050624 train.py:173] global step 003600: loss = 1.5982 (0.007 sec/step)\n","I0717 03:42:34.791933 140446799050624 train.py:173] global step 003800: loss = 1.5665 (0.007 sec/step)\n","I0717 03:42:35.765769 140446799050624 train.py:159] Epoch 010 Step 003907 - train_Accuracy : 53.96%  val_Accuracy : 58.48%\n","I0717 03:42:36.419211 140446799050624 train.py:173] global step 004000: loss = 1.5762 (0.007 sec/step)\n","I0717 03:42:37.811189 140446799050624 train.py:173] global step 004200: loss = 1.5511 (0.007 sec/step)\n","I0717 03:42:38.715258 140446799050624 train.py:159] Epoch 011 Step 004297 - train_Accuracy : 54.94%  val_Accuracy : 57.63%\n","I0717 03:42:39.430216 140446799050624 train.py:173] global step 004400: loss = 1.5453 (0.007 sec/step)\n","I0717 03:42:40.819754 140446799050624 train.py:173] global step 004600: loss = 1.5237 (0.007 sec/step)\n","I0717 03:42:41.655462 140446799050624 train.py:159] Epoch 012 Step 004688 - train_Accuracy : 55.64%  val_Accuracy : 59.38%\n","I0717 03:42:42.436220 140446799050624 train.py:173] global step 004800: loss = 1.4997 (0.007 sec/step)\n","I0717 03:42:43.823750 140446799050624 train.py:173] global step 005000: loss = 1.4958 (0.007 sec/step)\n","I0717 03:42:44.614984 140446799050624 train.py:159] Epoch 013 Step 005079 - train_Accuracy : 56.58%  val_Accuracy : 59.81%\n","I0717 03:42:45.453315 140446799050624 train.py:173] global step 005200: loss = 1.4611 (0.007 sec/step)\n","I0717 03:42:46.829283 140446799050624 train.py:173] global step 005400: loss = 1.4653 (0.007 sec/step)\n","I0717 03:42:47.530489 140446799050624 train.py:159] Epoch 014 Step 005469 - train_Accuracy : 57.41%  val_Accuracy : 61.58%\n","I0717 03:42:48.443590 140446799050624 train.py:173] global step 005600: loss = 1.4517 (0.007 sec/step)\n","I0717 03:42:49.821086 140446799050624 train.py:173] global step 005800: loss = 1.4426 (0.007 sec/step)\n","I0717 03:42:50.463555 140446799050624 train.py:159] Epoch 015 Step 005860 - train_Accuracy : 58.45%  val_Accuracy : 62.59%\n","I0717 03:42:51.427557 140446799050624 train.py:173] global step 006000: loss = 1.4271 (0.007 sec/step)\n","I0717 03:42:52.806438 140446799050624 train.py:173] global step 006200: loss = 1.4334 (0.007 sec/step)\n","I0717 03:42:53.378997 140446799050624 train.py:159] Epoch 016 Step 006250 - train_Accuracy : 58.93%  val_Accuracy : 61.92%\n","I0717 03:42:54.437572 140446799050624 train.py:173] global step 006400: loss = 1.3891 (0.007 sec/step)\n","I0717 03:42:55.850173 140446799050624 train.py:173] global step 006600: loss = 1.3928 (0.007 sec/step)\n","I0717 03:42:56.370292 140446799050624 train.py:159] Epoch 017 Step 006641 - train_Accuracy : 59.71%  val_Accuracy : 63.43%\n","I0717 03:42:57.483129 140446799050624 train.py:173] global step 006800: loss = 1.3954 (0.007 sec/step)\n","I0717 03:42:58.871899 140446799050624 train.py:173] global step 007000: loss = 1.3892 (0.007 sec/step)\n","I0717 03:42:59.324963 140446799050624 train.py:159] Epoch 018 Step 007032 - train_Accuracy : 60.32%  val_Accuracy : 63.46%\n","I0717 03:43:00.501118 140446799050624 train.py:173] global step 007200: loss = 1.3581 (0.007 sec/step)\n","I0717 03:43:01.893982 140446799050624 train.py:173] global step 007400: loss = 1.3798 (0.007 sec/step)\n","I0717 03:43:02.281903 140446799050624 train.py:159] Epoch 019 Step 007422 - train_Accuracy : 60.64%  val_Accuracy : 64.59%\n","I0717 03:43:03.536735 140446799050624 train.py:173] global step 007600: loss = 1.3532 (0.007 sec/step)\n","I0717 03:43:04.938755 140446799050624 train.py:173] global step 007800: loss = 1.3576 (0.007 sec/step)\n","I0717 03:43:05.258580 140446799050624 train.py:159] Epoch 020 Step 007813 - train_Accuracy : 61.20%  val_Accuracy : 63.75%\n","I0717 03:43:06.540731 140446799050624 train.py:173] global step 008000: loss = 1.3355 (0.007 sec/step)\n","I0717 03:43:07.924094 140446799050624 train.py:173] global step 008200: loss = 1.3214 (0.007 sec/step)\n","I0717 03:43:08.173525 140446799050624 train.py:159] Epoch 021 Step 008204 - train_Accuracy : 62.05%  val_Accuracy : 64.90%\n","I0717 03:43:09.526832 140446799050624 train.py:173] global step 008400: loss = 1.3123 (0.007 sec/step)\n","I0717 03:43:11.089250 140446799050624 train.py:159] Epoch 022 Step 008594 - train_Accuracy : 62.42%  val_Accuracy : 65.94%\n","I0717 03:43:11.130419 140446799050624 train.py:173] global step 008600: loss = 1.3250 (0.007 sec/step)\n","I0717 03:43:12.499944 140446799050624 train.py:173] global step 008800: loss = 1.2933 (0.007 sec/step)\n","I0717 03:43:13.991115 140446799050624 train.py:159] Epoch 023 Step 008985 - train_Accuracy : 62.97%  val_Accuracy : 66.55%\n","I0717 03:43:14.092545 140446799050624 train.py:173] global step 009000: loss = 1.2924 (0.007 sec/step)\n","I0717 03:43:15.462902 140446799050624 train.py:173] global step 009200: loss = 1.2714 (0.007 sec/step)\n","I0717 03:43:16.905817 140446799050624 train.py:159] Epoch 024 Step 009375 - train_Accuracy : 63.69%  val_Accuracy : 66.53%\n","I0717 03:43:17.079777 140446799050624 train.py:173] global step 009400: loss = 1.2944 (0.007 sec/step)\n","I0717 03:43:18.451812 140446799050624 train.py:173] global step 009600: loss = 1.2693 (0.007 sec/step)\n","I0717 03:43:19.818817 140446799050624 train.py:159] Epoch 025 Step 009766 - train_Accuracy : 63.99%  val_Accuracy : 67.04%\n","I0717 03:43:20.055428 140446799050624 train.py:173] global step 009800: loss = 1.2883 (0.007 sec/step)\n","I0717 03:43:21.443404 140446799050624 train.py:173] global step 010000: loss = 1.2510 (0.007 sec/step)\n","I0717 03:43:22.770020 140446799050624 train.py:159] Epoch 026 Step 010157 - train_Accuracy : 64.39%  val_Accuracy : 67.42%\n","I0717 03:43:23.078415 140446799050624 train.py:173] global step 010200: loss = 1.2453 (0.007 sec/step)\n","I0717 03:43:24.463212 140446799050624 train.py:173] global step 010400: loss = 1.2391 (0.007 sec/step)\n","I0717 03:43:25.715490 140446799050624 train.py:159] Epoch 027 Step 010547 - train_Accuracy : 64.89%  val_Accuracy : 67.64%\n","I0717 03:43:26.085820 140446799050624 train.py:173] global step 010600: loss = 1.2435 (0.007 sec/step)\n","I0717 03:43:27.467150 140446799050624 train.py:173] global step 010800: loss = 1.2469 (0.007 sec/step)\n","I0717 03:43:28.635745 140446799050624 train.py:159] Epoch 028 Step 010938 - train_Accuracy : 65.52%  val_Accuracy : 68.37%\n","I0717 03:43:29.067159 140446799050624 train.py:173] global step 011000: loss = 1.2333 (0.007 sec/step)\n","I0717 03:43:30.439127 140446799050624 train.py:173] global step 011200: loss = 1.2228 (0.007 sec/step)\n","I0717 03:43:31.550640 140446799050624 train.py:159] Epoch 029 Step 011329 - train_Accuracy : 65.46%  val_Accuracy : 68.08%\n","I0717 03:43:32.045712 140446799050624 train.py:173] global step 011400: loss = 1.2152 (0.007 sec/step)\n","I0717 03:43:33.422806 140446799050624 train.py:173] global step 011600: loss = 1.2173 (0.007 sec/step)\n","I0717 03:43:34.461312 140446799050624 train.py:159] Epoch 030 Step 011719 - train_Accuracy : 66.27%  val_Accuracy : 68.88%\n","I0717 03:43:35.026640 140446799050624 train.py:173] global step 011800: loss = 1.2034 (0.007 sec/step)\n","I0717 03:43:36.406589 140446799050624 train.py:173] global step 012000: loss = 1.1648 (0.007 sec/step)\n","I0717 03:43:37.405946 140446799050624 train.py:159] Epoch 031 Step 012110 - train_Accuracy : 67.83%  val_Accuracy : 69.65%\n","I0717 03:43:38.026195 140446799050624 train.py:173] global step 012200: loss = 1.1549 (0.007 sec/step)\n","I0717 03:43:39.406447 140446799050624 train.py:173] global step 012400: loss = 1.1526 (0.007 sec/step)\n","I0717 03:43:40.315858 140446799050624 train.py:159] Epoch 032 Step 012500 - train_Accuracy : 68.00%  val_Accuracy : 69.76%\n","I0717 03:43:41.006164 140446799050624 train.py:173] global step 012600: loss = 1.1572 (0.007 sec/step)\n","I0717 03:43:42.383848 140446799050624 train.py:173] global step 012800: loss = 1.1411 (0.007 sec/step)\n","I0717 03:43:43.241011 140446799050624 train.py:159] Epoch 033 Step 012891 - train_Accuracy : 68.53%  val_Accuracy : 69.92%\n","I0717 03:43:43.994047 140446799050624 train.py:173] global step 013000: loss = 1.1334 (0.007 sec/step)\n","I0717 03:43:45.381778 140446799050624 train.py:173] global step 013200: loss = 1.1515 (0.007 sec/step)\n","I0717 03:43:46.167996 140446799050624 train.py:159] Epoch 034 Step 013282 - train_Accuracy : 68.19%  val_Accuracy : 70.14%\n","I0717 03:43:46.983004 140446799050624 train.py:173] global step 013400: loss = 1.1404 (0.007 sec/step)\n","I0717 03:43:48.382976 140446799050624 train.py:173] global step 013600: loss = 1.1488 (0.007 sec/step)\n","I0717 03:43:49.112732 140446799050624 train.py:159] Epoch 035 Step 013672 - train_Accuracy : 68.24%  val_Accuracy : 70.18%\n","I0717 03:43:50.001365 140446799050624 train.py:173] global step 013800: loss = 1.1423 (0.007 sec/step)\n","I0717 03:43:51.385018 140446799050624 train.py:173] global step 014000: loss = 1.1544 (0.007 sec/step)\n","I0717 03:43:52.045990 140446799050624 train.py:159] Epoch 036 Step 014063 - train_Accuracy : 68.48%  val_Accuracy : 70.28%\n","I0717 03:43:52.995222 140446799050624 train.py:173] global step 014200: loss = 1.1342 (0.007 sec/step)\n","I0717 03:43:54.389042 140446799050624 train.py:173] global step 014400: loss = 1.1519 (0.007 sec/step)\n","I0717 03:43:54.998924 140446799050624 train.py:159] Epoch 037 Step 014454 - train_Accuracy : 68.30%  val_Accuracy : 70.22%\n","I0717 03:43:56.011299 140446799050624 train.py:173] global step 014600: loss = 1.1414 (0.007 sec/step)\n","I0717 03:43:57.394126 140446799050624 train.py:173] global step 014800: loss = 1.1504 (0.007 sec/step)\n","I0717 03:43:57.924487 140446799050624 train.py:159] Epoch 038 Step 014844 - train_Accuracy : 68.42%  val_Accuracy : 70.56%\n","I0717 03:43:59.025121 140446799050624 train.py:173] global step 015000: loss = 1.1338 (0.007 sec/step)\n","I0717 03:44:00.441499 140446799050624 train.py:173] global step 015200: loss = 1.1469 (0.007 sec/step)\n","I0717 03:44:00.921800 140446799050624 train.py:159] Epoch 039 Step 015235 - train_Accuracy : 68.71%  val_Accuracy : 70.50%\n","I0717 03:44:02.065411 140446799050624 train.py:173] global step 015400: loss = 1.1343 (0.007 sec/step)\n","I0717 03:44:03.455590 140446799050624 train.py:173] global step 015600: loss = 1.1420 (0.007 sec/step)\n","I0717 03:44:03.859820 140446799050624 train.py:159] Epoch 040 Step 015625 - train_Accuracy : 68.68%  val_Accuracy : 70.30%\n","I0717 03:44:05.072075 140446799050624 train.py:173] global step 015800: loss = 1.1348 (0.007 sec/step)\n","I0717 03:44:06.449111 140446799050624 train.py:173] global step 016000: loss = 1.1370 (0.007 sec/step)\n","I0717 03:44:06.783421 140446799050624 train.py:159] Epoch 041 Step 016016 - train_Accuracy : 68.66%  val_Accuracy : 70.76%\n","I0717 03:44:08.059601 140446799050624 train.py:173] global step 016200: loss = 1.1220 (0.007 sec/step)\n","I0717 03:44:09.455199 140446799050624 train.py:173] global step 016400: loss = 1.1444 (0.007 sec/step)\n","I0717 03:44:09.729430 140446799050624 train.py:159] Epoch 042 Step 016407 - train_Accuracy : 68.76%  val_Accuracy : 70.60%\n","I0717 03:44:11.067643 140446799050624 train.py:173] global step 016600: loss = 1.1215 (0.007 sec/step)\n","I0717 03:44:12.639454 140446799050624 train.py:159] Epoch 043 Step 016797 - train_Accuracy : 68.97%  val_Accuracy : 70.56%\n","I0717 03:44:12.659900 140446799050624 train.py:173] global step 016800: loss = 1.1367 (0.007 sec/step)\n","I0717 03:44:14.046622 140446799050624 train.py:173] global step 017000: loss = 1.1211 (0.007 sec/step)\n","I0717 03:44:15.567575 140446799050624 train.py:159] Epoch 044 Step 017188 - train_Accuracy : 68.84%  val_Accuracy : 70.71%\n","I0717 03:44:15.650360 140446799050624 train.py:173] global step 017200: loss = 1.1400 (0.007 sec/step)\n","I0717 03:44:17.032875 140446799050624 train.py:173] global step 017400: loss = 1.1322 (0.007 sec/step)\n","I0717 03:44:18.492015 140446799050624 train.py:159] Epoch 045 Step 017579 - train_Accuracy : 68.92%  val_Accuracy : 70.72%\n","I0717 03:44:18.636407 140446799050624 train.py:173] global step 017600: loss = 1.1339 (0.007 sec/step)\n","I0717 03:44:20.020262 140446799050624 train.py:173] global step 017800: loss = 1.1329 (0.007 sec/step)\n","I0717 03:44:21.405627 140446799050624 train.py:159] Epoch 046 Step 017969 - train_Accuracy : 68.93%  val_Accuracy : 70.78%\n","I0717 03:44:21.623776 140446799050624 train.py:173] global step 018000: loss = 1.1310 (0.007 sec/step)\n","I0717 03:44:23.010345 140446799050624 train.py:173] global step 018200: loss = 1.1350 (0.007 sec/step)\n","I0717 03:44:24.346029 140446799050624 train.py:159] Epoch 047 Step 018360 - train_Accuracy : 68.63%  val_Accuracy : 70.64%\n","I0717 03:44:24.621621 140446799050624 train.py:173] global step 018400: loss = 1.1404 (0.007 sec/step)\n","I0717 03:44:25.988832 140446799050624 train.py:173] global step 018600: loss = 1.1260 (0.007 sec/step)\n","I0717 03:44:27.243562 140446799050624 train.py:159] Epoch 048 Step 018750 - train_Accuracy : 69.26%  val_Accuracy : 71.06%\n","I0717 03:44:27.595501 140446799050624 train.py:173] global step 018800: loss = 1.1319 (0.007 sec/step)\n","I0717 03:44:28.987121 140446799050624 train.py:173] global step 019000: loss = 1.1223 (0.007 sec/step)\n","I0717 03:44:30.184162 140446799050624 train.py:159] Epoch 049 Step 019141 - train_Accuracy : 69.04%  val_Accuracy : 70.99%\n","I0717 03:44:30.600529 140446799050624 train.py:173] global step 019200: loss = 1.1332 (0.007 sec/step)\n","I0717 03:44:31.976077 140446799050624 train.py:173] global step 019400: loss = 1.1316 (0.007 sec/step)\n","I0717 03:44:33.108577 140446799050624 train.py:159] Epoch 050 Step 019532 - train_Accuracy : 69.09%  val_Accuracy : 70.88%\n","I0717 03:44:33.584495 140446799050624 train.py:173] global step 019600: loss = 1.1221 (0.007 sec/step)\n","I0717 03:44:34.959348 140446799050624 train.py:173] global step 019800: loss = 1.1146 (0.007 sec/step)\n","I0717 03:44:36.029318 140446799050624 train.py:159] Epoch 051 Step 019922 - train_Accuracy : 69.28%  val_Accuracy : 71.07%\n","I0717 03:44:36.572058 140446799050624 train.py:173] global step 020000: loss = 1.1303 (0.007 sec/step)\n","I0717 03:44:37.942683 140446799050624 train.py:173] global step 020200: loss = 1.1254 (0.007 sec/step)\n","I0717 03:44:38.942219 140446799050624 train.py:159] Epoch 052 Step 020313 - train_Accuracy : 68.88%  val_Accuracy : 70.87%\n","I0717 03:44:39.551325 140446799050624 train.py:173] global step 020400: loss = 1.1302 (0.007 sec/step)\n","I0717 03:44:40.922421 140446799050624 train.py:173] global step 020600: loss = 1.1167 (0.007 sec/step)\n","I0717 03:44:41.860546 140446799050624 train.py:159] Epoch 053 Step 020704 - train_Accuracy : 69.07%  val_Accuracy : 71.36%\n","I0717 03:44:42.519944 140446799050624 train.py:173] global step 020800: loss = 1.1197 (0.007 sec/step)\n","I0717 03:44:43.903395 140446799050624 train.py:173] global step 021000: loss = 1.1205 (0.007 sec/step)\n","I0717 03:44:44.769680 140446799050624 train.py:159] Epoch 054 Step 021094 - train_Accuracy : 69.27%  val_Accuracy : 70.73%\n","I0717 03:44:45.499660 140446799050624 train.py:173] global step 021200: loss = 1.1179 (0.007 sec/step)\n","I0717 03:44:46.873634 140446799050624 train.py:173] global step 021400: loss = 1.1230 (0.007 sec/step)\n","I0717 03:44:47.683756 140446799050624 train.py:159] Epoch 055 Step 021485 - train_Accuracy : 68.98%  val_Accuracy : 70.99%\n","I0717 03:44:48.481101 140446799050624 train.py:173] global step 021600: loss = 1.1199 (0.007 sec/step)\n","I0717 03:44:49.866853 140446799050624 train.py:173] global step 021800: loss = 1.1408 (0.007 sec/step)\n","I0717 03:44:50.602429 140446799050624 train.py:159] Epoch 056 Step 021875 - train_Accuracy : 68.78%  val_Accuracy : 71.13%\n","I0717 03:44:51.470724 140446799050624 train.py:173] global step 022000: loss = 1.1233 (0.007 sec/step)\n","I0717 03:44:52.847597 140446799050624 train.py:173] global step 022200: loss = 1.1299 (0.007 sec/step)\n","I0717 03:44:53.525784 140446799050624 train.py:159] Epoch 057 Step 022266 - train_Accuracy : 69.08%  val_Accuracy : 70.75%\n","I0717 03:44:54.446695 140446799050624 train.py:173] global step 022400: loss = 1.1098 (0.007 sec/step)\n","I0717 03:44:55.821703 140446799050624 train.py:173] global step 022600: loss = 1.1234 (0.007 sec/step)\n","I0717 03:44:56.438795 140446799050624 train.py:159] Epoch 058 Step 022657 - train_Accuracy : 69.04%  val_Accuracy : 71.25%\n","I0717 03:44:57.422454 140446799050624 train.py:173] global step 022800: loss = 1.1131 (0.007 sec/step)\n","I0717 03:44:58.800867 140446799050624 train.py:173] global step 023000: loss = 1.1282 (0.007 sec/step)\n","I0717 03:44:59.348648 140446799050624 train.py:159] Epoch 059 Step 023047 - train_Accuracy : 69.55%  val_Accuracy : 71.28%\n","I0717 03:45:00.404692 140446799050624 train.py:173] global step 023200: loss = 1.1047 (0.007 sec/step)\n","I0717 03:45:01.780240 140446799050624 train.py:173] global step 023400: loss = 1.1254 (0.007 sec/step)\n","I0717 03:45:02.264417 140446799050624 train.py:159] Epoch 060 Step 023438 - train_Accuracy : 69.53%  val_Accuracy : 71.32%\n","I0717 03:45:03.385468 140446799050624 train.py:173] global step 023600: loss = 1.1232 (0.007 sec/step)\n","I0717 03:45:04.761471 140446799050624 train.py:173] global step 023800: loss = 1.1142 (0.007 sec/step)\n","I0717 03:45:05.194522 140446799050624 train.py:159] Epoch 061 Step 023829 - train_Accuracy : 69.36%  val_Accuracy : 71.43%\n","I0717 03:45:06.382971 140446799050624 train.py:173] global step 024000: loss = 1.1195 (0.007 sec/step)\n","I0717 03:45:07.757924 140446799050624 train.py:173] global step 024200: loss = 1.1132 (0.007 sec/step)\n","I0717 03:45:08.124421 140446799050624 train.py:159] Epoch 062 Step 024219 - train_Accuracy : 69.39%  val_Accuracy : 71.50%\n","I0717 03:45:09.386306 140446799050624 train.py:173] global step 024400: loss = 1.1081 (0.007 sec/step)\n","I0717 03:45:10.762279 140446799050624 train.py:173] global step 024600: loss = 1.1091 (0.007 sec/step)\n","I0717 03:45:11.058880 140446799050624 train.py:159] Epoch 063 Step 024610 - train_Accuracy : 69.81%  val_Accuracy : 71.28%\n","I0717 03:45:12.392321 140446799050624 train.py:173] global step 024800: loss = 1.1174 (0.007 sec/step)\n","I0717 03:45:13.999798 140446799050624 train.py:159] Epoch 064 Step 025000 - train_Accuracy : 69.43%  val_Accuracy : 71.23%\n","I0717 03:45:14.000593 140446799050624 train.py:173] global step 025000: loss = 1.1073 (0.007 sec/step)\n","I0717 03:45:15.379576 140446799050624 train.py:173] global step 025200: loss = 1.1096 (0.007 sec/step)\n","I0717 03:45:16.913904 140446799050624 train.py:159] Epoch 065 Step 025391 - train_Accuracy : 69.98%  val_Accuracy : 71.48%\n","I0717 03:45:16.976394 140446799050624 train.py:173] global step 025400: loss = 1.1003 (0.007 sec/step)\n","I0717 03:45:18.362023 140446799050624 train.py:173] global step 025600: loss = 1.0976 (0.007 sec/step)\n","I0717 03:45:19.839338 140446799050624 train.py:159] Epoch 066 Step 025782 - train_Accuracy : 69.63%  val_Accuracy : 71.56%\n","I0717 03:45:19.960906 140446799050624 train.py:173] global step 025800: loss = 1.1168 (0.007 sec/step)\n","I0717 03:45:21.329810 140446799050624 train.py:173] global step 026000: loss = 1.1074 (0.007 sec/step)\n","I0717 03:45:22.744376 140446799050624 train.py:159] Epoch 067 Step 026172 - train_Accuracy : 69.76%  val_Accuracy : 71.51%\n","I0717 03:45:22.935142 140446799050624 train.py:173] global step 026200: loss = 1.1054 (0.007 sec/step)\n","I0717 03:45:24.314549 140446799050624 train.py:173] global step 026400: loss = 1.1201 (0.007 sec/step)\n","I0717 03:45:25.662592 140446799050624 train.py:159] Epoch 068 Step 026563 - train_Accuracy : 69.84%  val_Accuracy : 71.39%\n","I0717 03:45:25.917619 140446799050624 train.py:173] global step 026600: loss = 1.0901 (0.007 sec/step)\n","I0717 03:45:27.294235 140446799050624 train.py:173] global step 026800: loss = 1.1063 (0.007 sec/step)\n","I0717 03:45:28.592708 140446799050624 train.py:159] Epoch 069 Step 026954 - train_Accuracy : 70.04%  val_Accuracy : 71.45%\n","I0717 03:45:28.913068 140446799050624 train.py:173] global step 027000: loss = 1.0990 (0.007 sec/step)\n","I0717 03:45:30.291927 140446799050624 train.py:173] global step 027200: loss = 1.1182 (0.007 sec/step)\n","I0717 03:45:31.502680 140446799050624 train.py:159] Epoch 070 Step 027344 - train_Accuracy : 69.97%  val_Accuracy : 71.32%\n","I0717 03:45:31.899798 140446799050624 train.py:173] global step 027400: loss = 1.0910 (0.007 sec/step)\n","I0717 03:45:33.273422 140446799050624 train.py:173] global step 027600: loss = 1.1165 (0.007 sec/step)\n","I0717 03:45:34.428996 140446799050624 train.py:159] Epoch 071 Step 027735 - train_Accuracy : 69.73%  val_Accuracy : 71.46%\n","I0717 03:45:34.887045 140446799050624 train.py:173] global step 027800: loss = 1.1054 (0.007 sec/step)\n","I0717 03:45:36.266733 140446799050624 train.py:173] global step 028000: loss = 1.1213 (0.007 sec/step)\n","I0717 03:45:37.351131 140446799050624 train.py:159] Epoch 072 Step 028125 - train_Accuracy : 69.40%  val_Accuracy : 71.49%\n","I0717 03:45:37.872260 140446799050624 train.py:173] global step 028200: loss = 1.1012 (0.007 sec/step)\n","I0717 03:45:39.250073 140446799050624 train.py:173] global step 028400: loss = 1.1077 (0.007 sec/step)\n","I0717 03:45:40.263413 140446799050624 train.py:159] Epoch 073 Step 028516 - train_Accuracy : 69.87%  val_Accuracy : 71.46%\n","I0717 03:45:40.847100 140446799050624 train.py:173] global step 028600: loss = 1.1050 (0.007 sec/step)\n","I0717 03:45:42.224882 140446799050624 train.py:173] global step 028800: loss = 1.1051 (0.007 sec/step)\n","I0717 03:45:43.192792 140446799050624 train.py:159] Epoch 074 Step 028907 - train_Accuracy : 69.57%  val_Accuracy : 71.49%\n","I0717 03:45:43.833546 140446799050624 train.py:173] global step 029000: loss = 1.1144 (0.007 sec/step)\n","I0717 03:45:45.209068 140446799050624 train.py:173] global step 029200: loss = 1.1013 (0.007 sec/step)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JS82dKZffMeK","colab_type":"text"},"source":["![](https://drive.google.com/uc?id=1WCkdnD_aRTHrNU1UxrR26EEWPDUI5w8p)\n","\n","training result is not so good so you have to modify hyper-parameters.\n"]},{"cell_type":"markdown","metadata":{"id":"1e6W7G9Zc_8r","colab_type":"text"},"source":["## Modifying Code\n","If you want to modify just some of the hyper-parameters, re-uploading codes are so uncomfortable.\n","\n","so we need some text editor that can modify the codes in Google drive.\n","\n","Note that the codes have to be placed in your drive.\n","\n","![](https://drive.google.com/uc?id=1T3BvM1kPUBiOFePmebcS65oMCkW8cdMC)\n","\n","Text Editor for Drive is suitable for this, I think.\n","\n","it can access your Google Drive and edit the codes.\n","\n","So I modify some hyper-parameters and run again.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"vIu9zBxPenmH","colab_type":"code","outputId":"58ae9b34-2a3a-4f37-b3d3-ee75afb5c694","colab":{"base_uri":"https://localhost:8080/","height":1381}},"source":["!python /content/gdrive/My\\ Drive/testing_folder/tensorflow_classifier/train.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","2019-04-24 17:53:03.468149: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n","2019-04-24 17:53:03.468353: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2be55a0 executing computations on platform Host. Devices:\n","2019-04-24 17:53:03.468382: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n","2019-04-24 17:53:03.619201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-04-24 17:53:03.619705: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2be52e0 executing computations on platform CUDA. Devices:\n","2019-04-24 17:53:03.619733: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2019-04-24 17:53:03.620075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n","name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n","pciBusID: 0000:00:04.0\n","totalMemory: 14.73GiB freeMemory: 14.60GiB\n","2019-04-24 17:53:03.620099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n","2019-04-24 17:53:04.063532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2019-04-24 17:53:04.063598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n","2019-04-24 17:53:04.063624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n","2019-04-24 17:53:04.063902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n","network built\n","2019-04-24 17:53:04.480057: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n","Epoch 000 Step 000000 - train_Accuracy : 0.12%  val_Accuracy : 10.59%\n","Epoch 001 Step 000078 - train_Accuracy : 30.92%  val_Accuracy : 43.69%\n","global step 000100: loss = 2.0510 (0.038 sec/step)\n","Epoch 002 Step 000156 - train_Accuracy : 44.62%  val_Accuracy : 48.42%\n","global step 000200: loss = 1.6533 (0.016 sec/step)\n","Epoch 003 Step 000234 - train_Accuracy : 49.97%  val_Accuracy : 52.79%\n","global step 000300: loss = 1.4867 (0.017 sec/step)\n","Epoch 004 Step 000312 - train_Accuracy : 54.30%  val_Accuracy : 56.65%\n","Epoch 005 Step 000390 - train_Accuracy : 58.03%  val_Accuracy : 59.30%\n","global step 000400: loss = 1.3695 (0.016 sec/step)\n","Epoch 006 Step 000468 - train_Accuracy : 61.55%  val_Accuracy : 61.59%\n","global step 000500: loss = 1.2589 (0.016 sec/step)\n","Epoch 007 Step 000546 - train_Accuracy : 63.78%  val_Accuracy : 61.93%\n","global step 000600: loss = 1.1647 (0.016 sec/step)\n","Epoch 008 Step 000624 - train_Accuracy : 67.26%  val_Accuracy : 62.78%\n","global step 000700: loss = 1.0853 (0.017 sec/step)\n","Epoch 009 Step 000702 - train_Accuracy : 68.99%  val_Accuracy : 63.04%\n","Epoch 010 Step 000780 - train_Accuracy : 71.47%  val_Accuracy : 65.08%\n","global step 000800: loss = 0.9883 (0.016 sec/step)\n","Epoch 011 Step 000858 - train_Accuracy : 76.86%  val_Accuracy : 67.23%\n","global step 000900: loss = 0.8524 (0.016 sec/step)\n","Epoch 012 Step 000936 - train_Accuracy : 78.43%  val_Accuracy : 67.74%\n","global step 001000: loss = 0.8144 (0.017 sec/step)\n","Epoch 013 Step 001014 - train_Accuracy : 79.02%  val_Accuracy : 67.46%\n","Epoch 014 Step 001092 - train_Accuracy : 79.50%  val_Accuracy : 67.87%\n","global step 001100: loss = 0.7962 (0.017 sec/step)\n","Epoch 015 Step 001170 - train_Accuracy : 79.94%  val_Accuracy : 68.19%\n","global step 001200: loss = 0.7767 (0.017 sec/step)\n","Epoch 016 Step 001248 - train_Accuracy : 81.10%  val_Accuracy : 68.21%\n","global step 001300: loss = 0.7489 (0.016 sec/step)\n","Epoch 017 Step 001326 - train_Accuracy : 80.85%  val_Accuracy : 68.18%\n","global step 001400: loss = 0.7606 (0.017 sec/step)\n","Epoch 018 Step 001404 - train_Accuracy : 81.10%  val_Accuracy : 68.32%\n","Epoch 019 Step 001482 - train_Accuracy : 81.22%  val_Accuracy : 68.24%\n","global step 001500: loss = 0.7535 (0.017 sec/step)\n","Epoch 020 Step 001560 - train_Accuracy : 81.34%  val_Accuracy : 68.32%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rQGkNcjoeyom","colab_type":"text"},"source":["later one which called test0 is better than the former one\n","\n","![](https://drive.google.com/uc?id=1wfpIlwhfDrVMSLXG30PzXB-l38tF0RZB)\n","\n","and if you want to change logdir of Tensorboard, you have to kill running Tensorboard and rerun the Tensorboard\n","\n","all progress is explained in below codes"]},{"cell_type":"code","metadata":{"id":"3woZGfsvMkZZ","colab_type":"code","outputId":"b5e3f87b-ca81-475b-a96a-ce5fdd883fd7","executionInfo":{"status":"ok","timestamp":1556157201626,"user_tz":-540,"elapsed":2948,"user":{"displayName":"이승현","photoUrl":"","userId":"07009177712654713303"}},"colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["!ps -A"],"execution_count":0,"outputs":[{"output_type":"stream","text":["    PID TTY          TIME CMD\n","      1 ?        00:00:00 run.sh\n","     10 ?        00:00:00 node\n","     25 ?        00:00:00 node\n","     35 ?        00:00:01 jupyter-noteboo\n","    122 ?        00:00:00 tail\n","    130 ?        00:00:01 python3\n","    166 ?        00:00:00 drive\n","    248 ?        00:00:01 drive\n","    296 ?        00:00:00 tail\n","    297 ?        00:00:00 grep\n","    322 ?        00:00:02 tensorboard\n","    324 ?        00:00:01 ngrok\n","    592 ?        00:00:00 ps\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L__XJIRLMoP_","colab_type":"code","colab":{}},"source":["!kill -9 322"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8eDene_bMsDH","colab_type":"code","outputId":"17147cc9-aa66-4e14-8eaa-444d6004b412","executionInfo":{"status":"ok","timestamp":1556157235245,"user_tz":-540,"elapsed":3162,"user":{"displayName":"이승현","photoUrl":"","userId":"07009177712654713303"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["LOG_DIR = 'test/test1'\n","\n","get_ipython().system_raw(\n","    'tensorboard --logdir={} --host=0.0.0.0 --port=6006 &'\n","    .format(LOG_DIR))\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["http://769ca309.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"toec57lJNCMH","colab_type":"text"},"source":["That's all and enjoy you free GPU :)"]}]}